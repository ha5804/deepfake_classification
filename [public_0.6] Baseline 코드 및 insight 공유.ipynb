{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing + Baseline ViT Pipeline \n",
    "\n",
    "### pre-processing flow\n",
    "1. **face detecting** - Using dlib\n",
    "2. **landmark detecting** - 81개 랜드마크 중 5개 core points 추출\n",
    "3. **face alignment** - SimilarityTransform으로 정렬 후 224x224로 crop\n",
    "4. **face crop saving** - 500개 샘플 전부 저장(crop checking)\n",
    "5. **ViT model inference** - baseling model\n",
    "\n",
    "#### **env**  \n",
    "python == 3.9  \n",
    "dlib (conda install dlib)  \n",
    "torch == 2.8.0+cu128  \n",
    "이외 설치는 pip install 활용해 설치 (라이브러리 설치 error는 댓글로 문의.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/army/miniconda/envs/detector/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from skimage import transform as trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"prithivMLmods/Deep-Fake-Detector-v2-Model\"\n",
    "TEST_DIR = Path(\"./test_data\")\n",
    "\n",
    "# Landmark model path\n",
    "# Download from: https://huggingface.co/spaces/liangtian/birthdayCrown/blob/main/shape_predictor_81_face_landmarks.dat\n",
    "LANDMARK_MODEL_PATH = Path(\"./preprocessing/shape_predictor_81_face_landmarks.dat\")\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 전처리 이미지 저장 여부\n",
    "SAVE_CROPS = False  # If you wandt to save cropped face images\n",
    "\n",
    "# Cropped faces directory\n",
    "CROP_SAVE_DIR = OUTPUT_DIR / \"cropped_faces\"\n",
    "CROP_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (224, 224)  # Face crop\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dlib face detector and landmark predictor...\n",
      "Face detector and landmark predictor loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dlib models\n",
    "if not LANDMARK_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Landmark model not found: {LANDMARK_MODEL_PATH}\\n\"\n",
    "        \"Please download shape_predictor_81_face_landmarks.dat\"\n",
    "    )\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor(str(LANDMARK_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5_keypoints(image_rgb: np.ndarray, face: dlib.rectangle) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    81개 랜드마크에서 5개의 core point 추출\n",
    "    - left eye (#37), right eye (#44), nose (#30)\n",
    "    - left mouth (#49), right mouth (#55)\n",
    "    \"\"\"\n",
    "    shape = landmark_predictor(image_rgb, face)\n",
    "    \n",
    "    leye = np.array([shape.part(37).x, shape.part(37).y]).reshape(-1, 2)\n",
    "    reye = np.array([shape.part(44).x, shape.part(44).y]).reshape(-1, 2)\n",
    "    nose = np.array([shape.part(30).x, shape.part(30).y]).reshape(-1, 2)\n",
    "    lmouth = np.array([shape.part(49).x, shape.part(49).y]).reshape(-1, 2)\n",
    "    rmouth = np.array([shape.part(55).x, shape.part(55).y]).reshape(-1, 2)\n",
    "    \n",
    "    pts = np.concatenate([leye, reye, nose, lmouth, rmouth], axis=0)\n",
    "    return pts\n",
    "\n",
    "\n",
    "def align_and_crop_face(img_rgb: np.ndarray, landmarks: np.ndarray, \n",
    "                        outsize: Tuple[int, int] = (224, 224), \n",
    "                        scale: float = 1.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    5개 랜드마크를 사용하여 얼굴 정렬 및 crop\n",
    "    \"\"\"\n",
    "    target_size = [112, 112]\n",
    "    dst = np.array([\n",
    "        [30.2946, 51.6963],\n",
    "        [65.5318, 51.5014],\n",
    "        [48.0252, 71.7366],\n",
    "        [33.5493, 92.3655],\n",
    "        [62.7299, 92.2041]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    if target_size[1] == 112:\n",
    "        dst[:, 0] += 8.0\n",
    "\n",
    "    dst[:, 0] = dst[:, 0] * outsize[0] / target_size[0]\n",
    "    dst[:, 1] = dst[:, 1] * outsize[1] / target_size[1]\n",
    "\n",
    "    target_size = outsize\n",
    "\n",
    "    margin_rate = scale - 1\n",
    "    x_margin = target_size[0] * margin_rate / 2.\n",
    "    y_margin = target_size[1] * margin_rate / 2.\n",
    "\n",
    "    dst[:, 0] += x_margin\n",
    "    dst[:, 1] += y_margin\n",
    "\n",
    "    dst[:, 0] *= target_size[0] / (target_size[0] + 2 * x_margin)\n",
    "    dst[:, 1] *= target_size[1] / (target_size[1] + 2 * y_margin)\n",
    "\n",
    "    src = landmarks.astype(np.float32)\n",
    "\n",
    "    tform = trans.SimilarityTransform()\n",
    "    tform.estimate(src, dst)\n",
    "    M = tform.params[0:2, :]\n",
    "\n",
    "    aligned = cv2.warpAffine(img_rgb, M, (target_size[1], target_size[0]))\n",
    "    \n",
    "    if outsize is not None:\n",
    "        aligned = cv2.resize(aligned, (outsize[1], outsize[0]))\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "\n",
    "def extract_aligned_face_fast(img_rgb: np.ndarray, res: int = 224, scale: float = 0.8) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    얼굴 검출 및 정렬 (축소된 이미지에서 검출)\n",
    "    - scale: 이미지 축소 비율 (0.8 = 80% 크기로 축소) -> time cost 감소\n",
    "    - 얼굴이 없으면 None 반환\n",
    "    \"\"\"\n",
    "    small = cv2.resize(img_rgb, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    faces = face_detector(small, 1)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    \n",
    "    face = max(faces, key=lambda r: r.width() * r.height())\n",
    "    landmarks = get_5_keypoints(small, face)\n",
    "    aligned = align_and_crop_face(small, landmarks, outsize=(res, res))\n",
    "    \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils - Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = cv2.imread(str(file_path))\n",
    "            if img is None:\n",
    "                return []\n",
    "            return [cv2.cvtColor(img, cv2.COLOR_BGR2RGB)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Face Crop 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        face_imgs: List[Image.Image],\n",
    "        representative_face: Optional[np.ndarray] = None,\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.face_imgs = face_imgs  # PIL Images for inference\n",
    "        self.representative_face = representative_face  # representative face save (RGB numpy)\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "def preprocess_one_with_facecrop(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행 (얼굴 검출 + crop)\n",
    "    - 비디오: 여러 프레임에서 얼굴 검출, 대표 1장 저장\n",
    "    - 이미지: 1장에서 얼굴 검출\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "        \n",
    "        if not frames:\n",
    "            return PreprocessOutput(file_path.name, [], None, \"No frames extracted\")\n",
    "        \n",
    "        face_imgs: List[Image.Image] = []\n",
    "        representative_face: Optional[np.ndarray] = None\n",
    "        \n",
    "        for i, rgb in enumerate(frames):\n",
    "            aligned_face = extract_aligned_face_fast(rgb, res=224, scale=0.5)\n",
    "            \n",
    "            if aligned_face is not None:\n",
    "                face_imgs.append(Image.fromarray(aligned_face))\n",
    "                \n",
    "                if representative_face is None:\n",
    "                    representative_face = aligned_face\n",
    "        \n",
    "        if not face_imgs:\n",
    "            return PreprocessOutput(file_path.name, [], None, \"No face detected\")\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, face_imgs, representative_face, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], None, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing & Saving Face Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 500/500 [09:25<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "if SAVE_CROPS:\n",
    "    print(f\"Cropped faces will be saved to: {CROP_SAVE_DIR}\")\n",
    "\n",
    "preprocess_results: Dict[str, PreprocessOutput] = {}\n",
    "no_face_files: List[str] = []\n",
    "saved_count = 0\n",
    "\n",
    "for file_path in tqdm(files, desc=\"Preprocessing\"):\n",
    "    out = preprocess_one_with_facecrop(file_path)\n",
    "    preprocess_results[out.filename] = out\n",
    "\n",
    "    if out.error and \"No face\" in out.error:\n",
    "        no_face_files.append(out.filename)\n",
    "\n",
    "    if SAVE_CROPS and out.representative_face is not None:\n",
    "        save_name = Path(out.filename).stem + \".jpg\"\n",
    "        save_path = CROP_SAVE_DIR / save_name\n",
    "        cv2.imwrite(\n",
    "            str(save_path),\n",
    "            cv2.cvtColor(out.representative_face, cv2.COLOR_RGB2BGR)\n",
    "        )\n",
    "        saved_count += 1\n",
    "\n",
    "print(\"\\nPreprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif no_face_files:\\n    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\\n    for f in no_face_files[:30]:\\n        print(f\"  - {f}\")\\n    if len(no_face_files) > 30:\\n        print(f\"  ... and {len(no_face_files) - 30} more\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of failed files - If you want to see files with no detected faces, uncomment below\n",
    "'''\n",
    "if no_face_files:\n",
    "    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\n",
    "    for f in no_face_files[:30]:\n",
    "        print(f\"  - {f}\")\n",
    "    if len(no_face_files) > 30:\n",
    "        print(f\"  ... and {len(no_face_files) - 30} more\")\n",
    "'''\n",
    "# results : missing faces data = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    \"\"\"PIL 이미지 리스트에 대해 Fake probability 추론\"\"\"\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        batch_probs = F.softmax(logits, dim=1)[:, 1]  # Real probability (id2label: 0=Fake, 1=Real)\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 500/500 [00:05<00:00, 89.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results: Dict[str, float] = {}\n",
    "\n",
    "for filename, out in tqdm(preprocess_results.items(), desc=\"Inference\"):\n",
    "    if out.face_imgs:\n",
    "        probs = infer_fake_probs(out.face_imgs)\n",
    "        results[filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    else:\n",
    "        # 얼굴 검출 실패 시 0 (Real로 처리) -> basic logic\n",
    "        results[filename] = 0.0\n",
    "print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: output/submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
