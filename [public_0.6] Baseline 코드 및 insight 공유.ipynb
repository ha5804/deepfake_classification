{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing + Baseline ViT Pipeline \n",
    "\n",
    "### pre-processing flow\n",
    "1. **face detecting** - Using dlib\n",
    "2. **landmark detecting** - 81ê°œ ëœë“œë§ˆí¬ ì¤‘ 5ê°œ core points ì¶”ì¶œ\n",
    "3. **face alignment** - SimilarityTransformìœ¼ë¡œ ì •ë ¬ í›„ 224x224ë¡œ crop\n",
    "4. **face crop saving** - 500ê°œ ìƒ˜í”Œ ì „ë¶€ ì €ì¥(crop checking)\n",
    "5. **ViT model inference** - baseling model\n",
    "\n",
    "#### **env**  \n",
    "python == 3.9  \n",
    "dlib (conda install dlib)  \n",
    "torch == 2.8.0+cu128  \n",
    "ì´ì™¸ ì„¤ì¹˜ëŠ” pip install í™œìš©í•´ ì„¤ì¹˜ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ errorëŠ” ëŒ“ê¸€ë¡œ ë¬¸ì˜.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deepfake39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from skimage import transform as trans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "#ë„ì¼ ê²°ê³¼ ì¬í˜„, ì œì¶œ ì ìˆ˜ ë³€ë™ ë°©ì§€, ë””ë²„ê¹… í•„ìˆ˜\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# ì—°ì‚° ì†ë„ ë‚®ì¶”ê³  ê²°ê³¼ ê³ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"nateraw/xception-deepfake\"\n",
    "#huggingface hubì— ê³µê°œëœ ì‚¬ì „ í•™ìŠµëœ ë”¥í˜ì´í¬ ë¶„ë¥˜ ëª¨ë¸ ì§€ì •\n",
    "#ì´ë¯¸ ë‹¤ë¥¸ ì‚¬ëŒì´ ë”¥í˜ì´í¬ ë°ì´í„°ë¡œ í•™ìŠµí•´ë‘” ëª¨ë¸\n",
    "\n",
    "TEST_DIR = Path(\"/Users/taehayeong/Desktop/dataset-face/open/test_data\")\n",
    "#í‰ê°€ ë°ì´í„°ê°€ ì‹¤ì œë¡œ ë“¤ì–´ ìˆëŠ” í´ë” ìœ„ì¹˜ ì§€ì •\n",
    "\n",
    "# Landmark model path\n",
    "# Download from: https://huggingface.co/spaces/liangtian/birthdayCrown/blob/main/shape_predictor_81_face_landmarks.dat\n",
    "\n",
    "\n",
    "# LANDMARK_MODEL_PATH = Path(\"./preprocessing/shape_predictor_81_face_landmarks.dat\")\n",
    "#dlibì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì •ê¸° íŒŒì¼ ìœ„ì¹˜ ì–¼êµ´ì—ì„œ ëˆˆ ì½” ì… ì¢Œí‘œ ì¶”ì¶œì— ì‚¬ìš©\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#ê²°ê³¼ë¬¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„± subbmission.cs\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€\n",
    "SAVE_CROPS = True  # If you wandt to save cropped face images\n",
    "\n",
    "# Cropped faces directory\n",
    "CROP_SAVE_DIR = OUTPUT_DIR / \"cropped_faces\"\n",
    "CROP_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (224, 224)  # Face crop\n",
    "#ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224í•´ìƒë„ë¡œ ë§ì¶¤, ì´ ê°€ì •ì€ ë¯¸ì„¸í•œ ì†ì‹¤ ìœ ë°œ.\n",
    "\n",
    "NUM_FRAMES = 5  # ë¹„ë””ì˜¤ ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜\n",
    "#ì˜ìƒ 1ê°œë‹¹ ëª‡ í”„ë ˆì„ ë½‘ì„ì§€ ê²°ì •, ë„ˆë¬´ ë†’ìœ¼ë©´ ë”¥í˜ì´í¬í”„ë ˆì„ ë†“ì¹˜ê³ , ë§ìœ¼ë©´ ì¶”ë¡ ì‹œê°„ í­ë“±\n",
    "#ë”¥í˜ì´í¬ ë‹¨ì„œê°€ ì˜ìƒ ì „ì²´ì— ê³ ë¥´ê²Œ í¼ì ¸ìˆë‹¤ëŠ” ê°€ì •. ë”¥í˜ì´í¬ëŠ” ì‹¤ì œë¡œ ì¼ë¶€í”„ë ˆì„ ë§ê°€ì§\n",
    "#ë”°ë¼ì„œ ì„±ëŠ¥ì— ì˜í–¥ í´ í™•ë¥  ë†’ìŒ\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "#macì´ë©´ mps, ì™€ ê°™ì´ ë™ì¼í•˜ê²Œ ëŒì•„ê°€ê²Œ í•˜ëŠ” ëª©ì \n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "#ìœ„ì—ì„œ ì–´ë–¤ ì¥ì¹˜ ë§¥ë¶ or gpu ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "\n",
    "#cropì€ í° ì´ë¯¸ì§€ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ëŠ”ê²ƒ, ë”¥í˜ì´í¬ì—ì„œ ì¤‘ìš”í•œê±´ ì–¼êµ´\n",
    "#ì–¼êµ´ì„ cropí•œë‹¤ëŠ”ê±´ ë°°ê²½ì„ ì œê±°í•œë‹¤ëŠ”ê²ƒ.\n",
    "#cropì„¤ì •ì„ í†µí•´ ëª¨ë¸ì€ ì •ë©´ì„ ë°”ë¼ë³´ëŠ” ê°™ì€ í¬ê¸°ì˜ ì–¼êµ´ ì‚¬ì§„ë§Œ ë³´ê²Œ ëœë‹¤.\n",
    "#10ì´ˆ ì˜ìƒ ì´ˆë‹¹ 30í”„ë ˆì„ì´ë©´ 300ì¥ ì´ë¯¸ì§€ ì´ì–´ì§„ê²ƒ.\n",
    "#ì¦‰ ì˜ìƒì„ ì—¬ëŸ¬ì¥ì˜ ì´ë¯¸ì§€ ë¬¶ìŒìœ¼ë¡œ ë³´ëŠ”ê²ƒ.\n",
    "\n",
    "#!!!!!!!!!!í”„ë ˆì„ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í•©ì¹˜ê³  ì–¼êµ´ì„ ì–´ë–»ê²Œ ì˜ë¼ì„œ ë³´ì—¬ì£¼ëŠ”ì§€ ì¤‘ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769652466.954576       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1769652466.962634       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n"
     ]
    }
   ],
   "source": [
    "# Load MediaPipe face detection & face mesh models\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Face detector (BlazeFace)\n",
    "face_detector = mp_face_detection.FaceDetection(\n",
    "    model_selection=1,            # 0: short-range, 1: full-range (ì˜ìƒ/ë‹¤ì¤‘ ì–¼êµ´ì— ìœ ë¦¬)\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "# Face mesh (landmarks)\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,      # ì˜ìƒ/ì—°ì† í”„ë ˆì„ ê¸°ì¤€\n",
    "    max_num_faces=1,              # ë”¥í˜ì´í¬: ë³´í†µ ë‹¨ì¼ ì–¼êµ´\n",
    "    refine_landmarks=True,        # ëˆˆ/ì… ì •ë°€ë„ â†‘\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_bbox_only(\n",
    "    img_rgb: np.ndarray,\n",
    "    res: int = 224,\n",
    "    expand: float = 0.15   # ì–¼êµ´ ì£¼ë³€ ì—¬ìœ  (ì¤‘ìš”)\n",
    ") -> Optional[np.ndarray]:\n",
    "\n",
    "    h, w, _ = img_rgb.shape\n",
    "    results = face_detector.process(img_rgb)\n",
    "\n",
    "    if results.detections is None:\n",
    "        return None\n",
    "\n",
    "    det = max(\n",
    "        results.detections,\n",
    "        key=lambda d: d.location_data.relative_bounding_box.width *\n",
    "                      d.location_data.relative_bounding_box.height\n",
    "    )\n",
    "\n",
    "    bbox = det.location_data.relative_bounding_box\n",
    "\n",
    "    x1 = int((bbox.xmin - expand) * w)\n",
    "    y1 = int((bbox.ymin - expand) * h)\n",
    "    x2 = int((bbox.xmin + bbox.width + expand) * w)\n",
    "    y2 = int((bbox.ymin + bbox.height + expand) * h)\n",
    "\n",
    "    # clamp\n",
    "    x1 = max(0, x1); y1 = max(0, y1)\n",
    "    x2 = min(w, x2); y2 = min(h, y2)\n",
    "\n",
    "    face = img_rgb[y1:y2, x1:x2]\n",
    "    if face.size == 0:\n",
    "        return None\n",
    "\n",
    "    return cv2.resize(face, (res, res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_5_keypoints_from_mesh(\n",
    "#     face_landmarks,\n",
    "#     img_w: int,\n",
    "#     img_h: int\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     MediaPipe FaceMesh (468 landmarks)ì—ì„œ 5ê°œì˜ core point ì¶”ì¶œ\n",
    "#     - left eye, right eye, nose tip\n",
    "#     - left mouth, right mouth\n",
    "#     \"\"\"\n",
    "\n",
    "#     # MediaPipe landmark index (ê³ ì •)\n",
    "#     IDX = {\n",
    "#         \"leye\": 33,     # left eye outer\n",
    "#         \"reye\": 263,    # right eye outer\n",
    "#         \"nose\": 1,      # nose tip\n",
    "#         \"lmouth\": 61,   # left mouth corner\n",
    "#         \"rmouth\": 291   # right mouth corner\n",
    "#     }\n",
    "\n",
    "#     pts = []\n",
    "#     for k in [\"leye\", \"reye\", \"nose\", \"lmouth\", \"rmouth\"]:\n",
    "#         lm = face_landmarks.landmark[IDX[k]]\n",
    "#         pts.append([lm.x * img_w, lm.y * img_h])\n",
    "\n",
    "#     return np.array(pts, dtype=np.float32)\n",
    "\n",
    "\n",
    "# def align_and_crop_face(\n",
    "#     img_rgb: np.ndarray,\n",
    "#     landmarks: np.ndarray,\n",
    "#     outsize: Tuple[int, int] = (224, 224),\n",
    "#     scale: float = 1.3\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     5ê°œ ëœë“œë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ì •ë ¬ ë° crop\n",
    "#     (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "#     \"\"\"\n",
    "\n",
    "#     target_size = [112, 112]\n",
    "#     dst = np.array([\n",
    "#         [30.2946, 51.6963],\n",
    "#         [65.5318, 51.5014],\n",
    "#         [48.0252, 71.7366],\n",
    "#         [33.5493, 92.3655],\n",
    "#         [62.7299, 92.2041]\n",
    "#     ], dtype=np.float32)\n",
    "\n",
    "#     if target_size[1] == 112:\n",
    "#         dst[:, 0] += 8.0\n",
    "\n",
    "#     dst[:, 0] = dst[:, 0] * outsize[0] / target_size[0]\n",
    "#     dst[:, 1] = dst[:, 1] * outsize[1] / target_size[1]\n",
    "\n",
    "#     target_size = outsize\n",
    "\n",
    "#     margin_rate = scale - 1\n",
    "#     x_margin = target_size[0] * margin_rate / 2.\n",
    "#     y_margin = target_size[1] * margin_rate / 2.\n",
    "\n",
    "#     dst[:, 0] += x_margin\n",
    "#     dst[:, 1] += y_margin\n",
    "\n",
    "#     dst[:, 0] *= target_size[0] / (target_size[0] + 2 * x_margin)\n",
    "#     dst[:, 1] *= target_size[1] / (target_size[1] + 2 * y_margin)\n",
    "\n",
    "#     src = landmarks.astype(np.float32)\n",
    "\n",
    "#     tform = trans.SimilarityTransform()\n",
    "#     tform.estimate(src, dst)\n",
    "#     M = tform.params[0:2, :]\n",
    "\n",
    "#     aligned = cv2.warpAffine(img_rgb, M, (target_size[1], target_size[0]))\n",
    "\n",
    "#     if outsize is not None:\n",
    "#         aligned = cv2.resize(aligned, (outsize[1], outsize[0]))\n",
    "\n",
    "#     return aligned\n",
    "\n",
    "\n",
    "# def extract_aligned_face_fast(\n",
    "#     img_rgb: np.ndarray,\n",
    "#     res: int = 224,\n",
    "#     scale: float = 1.0\n",
    "# ) -> Optional[np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     MediaPipe ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ ë° ì •ë ¬\n",
    "#     - ì–¼êµ´ì´ ì—†ìœ¼ë©´ None ë°˜í™˜\n",
    "#     \"\"\"\n",
    "\n",
    "#     img_h, img_w, _ = img_rgb.shape\n",
    "\n",
    "#     # Face detection\n",
    "#     results = face_detector.process(img_rgb)\n",
    "\n",
    "#     if results.detections is None:\n",
    "#         return None\n",
    "\n",
    "#     # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ\n",
    "#     det = max(\n",
    "#         results.detections,\n",
    "#         key=lambda d: d.location_data.relative_bounding_box.width *\n",
    "#                       d.location_data.relative_bounding_box.height\n",
    "#     )\n",
    "\n",
    "#     # Face mesh (landmarks)\n",
    "#     mesh_results = face_mesh.process(img_rgb)\n",
    "#     if mesh_results.multi_face_landmarks is None:\n",
    "#         return None\n",
    "\n",
    "#     face_landmarks = mesh_results.multi_face_landmarks[0]\n",
    "\n",
    "#     landmarks = get_5_keypoints_from_mesh(\n",
    "#         face_landmarks,\n",
    "#         img_w,\n",
    "#         img_h\n",
    "#     )\n",
    "\n",
    "#     aligned = align_and_crop_face(\n",
    "#         img_rgb,\n",
    "#         landmarks,\n",
    "#         outsize=(res, res)\n",
    "#     )\n",
    "\n",
    "#     return aligned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils - Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def blur_score(frame_rgb: np.ndarray) -> float:\n",
    "    \"\"\"í”„ë ˆì„ ì„ ëª…ë„ ì¸¡ì • (í´ìˆ˜ë¡ ì„ ëª…)\"\"\"\n",
    "    gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def importance_frame_indices(frames, num_frames):\n",
    "    if len(frames) <= num_frames:\n",
    "        return np.arange(len(frames))\n",
    "\n",
    "    scores = []\n",
    "    for idx, frame in enumerate(frames):\n",
    "        sharpness = blur_score(frame)\n",
    "        scores.append((idx, sharpness))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    selected = [idx for idx, _ in scores[:num_frames]]\n",
    "    return np.array(sorted(selected))\n",
    "\n",
    "def read_rgb_frames_fast(file_path, num_frames):\n",
    "    cap = cv2.VideoCapture(str(file_path))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = uniform_frame_indices(total, num_frames)\n",
    "\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Face Crop ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        face_imgs: List[Image.Image],\n",
    "        representative_face: Optional[np.ndarray] = None,\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.face_imgs = face_imgs  # PIL Images for inference\n",
    "        self.representative_face = representative_face  # representative face save (RGB numpy)\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "def preprocess_one_with_facecrop(\n",
    "    file_path: Path,\n",
    "    num_frames: int = NUM_FRAMES\n",
    ") -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    íŒŒì¼ í•˜ë‚˜ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰ (MediaPipe ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ + crop)\n",
    "    - AUC ìµœì í™”ìš© ì¡°ê±´ë¶€ blur / face area í•„í„°\n",
    "    - importance frame selection (ì‹¤íŒ¨ ì‹œ ìœ ì§€)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames_fast(file_path, num_frames=num_frames)\n",
    "\n",
    "        if not frames:\n",
    "            return PreprocessOutput(\n",
    "                file_path.name, [], None, \"No frames extracted\"\n",
    "            )\n",
    "\n",
    "        # ğŸ”¥ importance ê¸°ë°˜ í”„ë ˆì„ ì„ íƒ (AUC ì•ˆì •í™”)\n",
    "        try:\n",
    "            k = min(5, len(frames))\n",
    "            indices = importance_frame_indices(frames, k)\n",
    "            selected_frames = [frames[i] for i in indices]\n",
    "        except Exception:\n",
    "            selected_frames = frames\n",
    "\n",
    "        # â— ì¤‘ìš”: ë„ˆë¬´ ì¤„ì–´ë“¤ë©´ ì›ë³¸ ìœ ì§€\n",
    "        if len(selected_frames) < 2:\n",
    "            selected_frames = frames\n",
    "\n",
    "        frames = selected_frames\n",
    "\n",
    "        face_imgs: List[Image.Image] = []\n",
    "        representative_face: Optional[np.ndarray] = None\n",
    "\n",
    "        for rgb in frames:\n",
    "            h, w, _ = rgb.shape\n",
    "\n",
    "            # 1ï¸âƒ£ face detection (area ê³„ì‚°ìš©)\n",
    "            det_res = face_detector.process(rgb)\n",
    "            if det_res.detections is None:\n",
    "                continue\n",
    "\n",
    "            det = max(\n",
    "                det_res.detections,\n",
    "                key=lambda d: (\n",
    "                    d.location_data.relative_bounding_box.width *\n",
    "                    d.location_data.relative_bounding_box.height\n",
    "                )\n",
    "            )\n",
    "\n",
    "            bbox = det.location_data.relative_bounding_box\n",
    "            face_area = (bbox.width * w) * (bbox.height * h)\n",
    "\n",
    "            # 2ï¸âƒ£ blur score\n",
    "            blur = blur_score(rgb)\n",
    "\n",
    "            # ğŸ”¥ AUCìš© ì¡°ê±´ë¶€ í•„í„° (ë‘˜ ë‹¤ ë‚˜ì  ë•Œë§Œ ì œê±°)\n",
    "            if blur < 50 and face_area < 0.05 * (h * w):\n",
    "                continue\n",
    "\n",
    "            # 3ï¸âƒ£ ì •ë ¬ + crop\n",
    "            aligned_face = extract_face_bbox_only(rgb, res=224)\n",
    "            if aligned_face is None:\n",
    "                continue\n",
    "\n",
    "            face_imgs.append(Image.fromarray(aligned_face))\n",
    "\n",
    "            if representative_face is None:\n",
    "                representative_face = aligned_face\n",
    "\n",
    "        # ğŸ”¥ fallback: ì „ë¶€ ì œê±°ëœ ê²½ìš° ìµœì†Œ 1í”„ë ˆì„ í™•ë³´\n",
    "        if not face_imgs:\n",
    "            for rgb in frames:\n",
    "                aligned_face = extract_face_bbox_only(rgb, res=224)\n",
    "                if aligned_face is not None:\n",
    "                    face_imgs.append(Image.fromarray(aligned_face))\n",
    "                    representative_face = aligned_face\n",
    "                    break\n",
    "\n",
    "        if not face_imgs:\n",
    "            return PreprocessOutput(\n",
    "                file_path.name, [], None, \"No face detected\"\n",
    "            )\n",
    "\n",
    "        return PreprocessOutput(\n",
    "            file_path.name,\n",
    "            face_imgs,\n",
    "            representative_face,\n",
    "            None\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(\n",
    "            file_path.name, [], None, str(e)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing & Saving Face Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 500\n",
      "Cropped faces will be saved to: output/cropped_faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:  12%|â–ˆâ–        | 59/500 [00:06<01:00,  7.29it/s][mjpeg @ 0x31126b890] unable to decode APP fields: Invalid data found when processing input\n",
      "[mjpeg @ 0x31126b890] unable to decode APP fields: Invalid data found when processing input\n",
      "[mjpeg @ 0x31126b890] unable to decode APP fields: Invalid data found when processing input\n",
      "[mjpeg @ 0x31126b890] unable to decode APP fields: Invalid data found when processing input\n",
      "Preprocessing:  24%|â–ˆâ–ˆâ–       | 122/500 [00:15<00:47,  7.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m saved_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_one_with_facecrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     preprocess_results[out\u001b[38;5;241m.\u001b[39mfilename] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo face\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39merror:\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mpreprocess_one_with_facecrop\u001b[0;34m(file_path, num_frames)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03míŒŒì¼ í•˜ë‚˜ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰ (MediaPipe ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ + crop)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m- AUC ìµœì í™”ìš© ì¡°ê±´ë¶€ blur / face area í•„í„°\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m- importance frame selection (ì‹¤íŒ¨ ì‹œ ìœ ì§€)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mread_rgb_frames_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m frames:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PreprocessOutput(\n\u001b[1;32m     29\u001b[0m             file_path\u001b[38;5;241m.\u001b[39mname, [], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo frames extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mread_rgb_frames_fast\u001b[0;34m(file_path, num_frames)\u001b[0m\n\u001b[1;32m     32\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "if SAVE_CROPS:\n",
    "    print(f\"Cropped faces will be saved to: {CROP_SAVE_DIR}\")\n",
    "\n",
    "preprocess_results: Dict[str, PreprocessOutput] = {}\n",
    "no_face_files: List[str] = []\n",
    "saved_count = 0\n",
    "\n",
    "for file_path in tqdm(files, desc=\"Preprocessing\"):\n",
    "    out = preprocess_one_with_facecrop(file_path)\n",
    "    preprocess_results[out.filename] = out\n",
    "\n",
    "    if out.error and \"No face\" in out.error:\n",
    "        no_face_files.append(out.filename)\n",
    "\n",
    "    if SAVE_CROPS and out.representative_face is not None:\n",
    "        save_name = Path(out.filename).stem + \".jpg\"\n",
    "        save_path = CROP_SAVE_DIR / save_name\n",
    "        cv2.imwrite(\n",
    "            str(save_path),\n",
    "            cv2.cvtColor(out.representative_face, cv2.COLOR_RGB2BGR)\n",
    "        )\n",
    "        saved_count += 1\n",
    "\n",
    "print(\"\\nPreprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif no_face_files:\\n    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\\n    for f in no_face_files[:30]:\\n        print(f\"  - {f}\")\\n    if len(no_face_files) > 30:\\n        print(f\"  ... and {len(no_face_files) - 30} more\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of failed files - If you want to see files with no detected faces, uncomment below\n",
    "'''\n",
    "if no_face_files:\n",
    "    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\n",
    "    for f in no_face_files[:30]:\n",
    "        print(f\"  - {f}\")\n",
    "    if len(no_face_files) > 30:\n",
    "        print(f\"  ... and {len(no_face_files) - 30} more\")\n",
    "'''\n",
    "# results : missing faces data = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    \"\"\"PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ Fake probability ì¶”ë¡ \"\"\"\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        # batch_probs = F.softmax(logits, dim=1)[:, 1]  # Real probability (id2label: 0=Fake, 1=Real)\n",
    "        batch_probs = 1.0 - F.softmax(logits, dim=1)[:, 1]\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:13<00:00, 35.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results: Dict[str, float] = {}\n",
    "\n",
    "for filename, out in tqdm(preprocess_results.items(), desc=\"Inference\"):\n",
    "    if out.face_imgs:\n",
    "        probs = infer_fake_probs(out.face_imgs)\n",
    "        # results[filename] = float(np.percentile(probs, 90)) if probs else 0.0\n",
    "        results[filename] = float(np.percentile(probs, 75))\n",
    "\n",
    "    else:\n",
    "        results[filename] = np.mean(list(results.values())) if results else 0.01\n",
    "\n",
    "print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: output/submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"/Users/taehayeong/Desktop/dataset-face/open/sample_submission.csv\")\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
