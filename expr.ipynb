{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing + Baseline ViT Pipeline \n",
    "\n",
    "### pre-processing flow\n",
    "1. **face detecting** - Using dlib\n",
    "2. **landmark detecting** - 81ê°œ ëœë“œë§ˆí¬ ì¤‘ 5ê°œ core points ì¶”ì¶œ\n",
    "3. **face alignment** - SimilarityTransformìœ¼ë¡œ ì •ë ¬ í›„ 224x224ë¡œ crop\n",
    "4. **face crop saving** - 500ê°œ ìƒ˜í”Œ ì „ë¶€ ì €ì¥(crop checking)\n",
    "5. **ViT model inference** - baseling model\n",
    "\n",
    "#### **env**  \n",
    "python == 3.9  \n",
    "dlib (conda install dlib)  \n",
    "torch == 2.8.0+cu128  \n",
    "ì´ì™¸ ì„¤ì¹˜ëŠ” pip install í™œìš©í•´ ì„¤ì¹˜ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ errorëŠ” ëŒ“ê¸€ë¡œ ë¬¸ì˜.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/deepfake39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from skimage import transform as trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "#ë„ì¼ ê²°ê³¼ ì¬í˜„, ì œì¶œ ì ìˆ˜ ë³€ë™ ë°©ì§€, ë””ë²„ê¹… í•„ìˆ˜\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# ì—°ì‚° ì†ë„ ë‚®ì¶”ê³  ê²°ê³¼ ê³ ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"prithivMLmods/Deep-Fake-Detector-v2-Model\"\n",
    "#huggingface hubì— ê³µê°œëœ ì‚¬ì „ í•™ìŠµëœ ë”¥í˜ì´í¬ ë¶„ë¥˜ ëª¨ë¸ ì§€ì •\n",
    "#ì´ë¯¸ ë‹¤ë¥¸ ì‚¬ëŒì´ ë”¥í˜ì´í¬ ë°ì´í„°ë¡œ í•™ìŠµí•´ë‘” ëª¨ë¸\n",
    "\n",
    "TEST_DIR = Path(\"/Users/taehayeong/Desktop/dataset-face/open/test_data\")\n",
    "#í‰ê°€ ë°ì´í„°ê°€ ì‹¤ì œë¡œ ë“¤ì–´ ìˆëŠ” í´ë” ìœ„ì¹˜ ì§€ì •\n",
    "\n",
    "# Landmark model path\n",
    "# Download from: https://huggingface.co/spaces/liangtian/birthdayCrown/blob/main/shape_predictor_81_face_landmarks.dat\n",
    "LANDMARK_MODEL_PATH = Path(\"./preprocessing/shape_predictor_81_face_landmarks.dat\")\n",
    "#dlibì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì •ê¸° íŒŒì¼ ìœ„ì¹˜ ì–¼êµ´ì—ì„œ ëˆˆ ì½” ì… ì¢Œí‘œ ì¶”ì¶œì— ì‚¬ìš©\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "#ê²°ê³¼ë¬¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„± subbmission.cs\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€\n",
    "SAVE_CROPS = False  # If you wandt to save cropped face images\n",
    "\n",
    "# Cropped faces directory\n",
    "CROP_SAVE_DIR = OUTPUT_DIR / \"cropped_faces\"\n",
    "CROP_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (224, 224)  # Face crop\n",
    "#ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 224í•´ìƒë„ë¡œ ë§ì¶¤, ì´ ê°€ì •ì€ ë¯¸ì„¸í•œ ì†ì‹¤ ìœ ë°œ.\n",
    "\n",
    "NUM_FRAMES = 20  # ë¹„ë””ì˜¤ ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜\n",
    "#ì˜ìƒ 1ê°œë‹¹ ëª‡ í”„ë ˆì„ ë½‘ì„ì§€ ê²°ì •, ë„ˆë¬´ ë†’ìœ¼ë©´ ë”¥í˜ì´í¬í”„ë ˆì„ ë†“ì¹˜ê³ , ë§ìœ¼ë©´ ì¶”ë¡ ì‹œê°„ í­ë“±\n",
    "#ë”¥í˜ì´í¬ ë‹¨ì„œê°€ ì˜ìƒ ì „ì²´ì— ê³ ë¥´ê²Œ í¼ì ¸ìˆë‹¤ëŠ” ê°€ì •. ë”¥í˜ì´í¬ëŠ” ì‹¤ì œë¡œ ì¼ë¶€í”„ë ˆì„ ë§ê°€ì§\n",
    "#ë”°ë¼ì„œ ì„±ëŠ¥ì— ì˜í–¥ í´ í™•ë¥  ë†’ìŒ\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "#macì´ë©´ mps, ì™€ ê°™ì´ ë™ì¼í•˜ê²Œ ëŒì•„ê°€ê²Œ í•˜ëŠ” ëª©ì \n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "#ìœ„ì—ì„œ ì–´ë–¤ ì¥ì¹˜ ë§¥ë¶ or gpu ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "\n",
    "#cropì€ í° ì´ë¯¸ì§€ì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ëŠ”ê²ƒ, ë”¥í˜ì´í¬ì—ì„œ ì¤‘ìš”í•œê±´ ì–¼êµ´\n",
    "#ì–¼êµ´ì„ cropí•œë‹¤ëŠ”ê±´ ë°°ê²½ì„ ì œê±°í•œë‹¤ëŠ”ê²ƒ.\n",
    "#cropì„¤ì •ì„ í†µí•´ ëª¨ë¸ì€ ì •ë©´ì„ ë°”ë¼ë³´ëŠ” ê°™ì€ í¬ê¸°ì˜ ì–¼êµ´ ì‚¬ì§„ë§Œ ë³´ê²Œ ëœë‹¤.\n",
    "#10ì´ˆ ì˜ìƒ ì´ˆë‹¹ 30í”„ë ˆì„ì´ë©´ 300ì¥ ì´ë¯¸ì§€ ì´ì–´ì§„ê²ƒ.\n",
    "#ì¦‰ ì˜ìƒì„ ì—¬ëŸ¬ì¥ì˜ ì´ë¯¸ì§€ ë¬¶ìŒìœ¼ë¡œ ë³´ëŠ”ê²ƒ.\n",
    "\n",
    "#!!!!!!!!!!í”„ë ˆì„ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í•©ì¹˜ê³  ì–¼êµ´ì„ ì–´ë–»ê²Œ ì˜ë¼ì„œ ë³´ì—¬ì£¼ëŠ”ì§€ ì¤‘ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection & Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dlib models\n",
    "if not LANDMARK_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Landmark model not found: {LANDMARK_MODEL_PATH}\\n\"\n",
    "        \"Please download shape_predictor_81_face_landmarks.dat\"\n",
    "    )\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor(str(LANDMARK_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5_keypoints(image_rgb: np.ndarray, face: dlib.rectangle) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    81ê°œ ëœë“œë§ˆí¬ì—ì„œ 5ê°œì˜ core point ì¶”ì¶œ\n",
    "    - left eye (#37), right eye (#44), nose (#30)\n",
    "    - left mouth (#49), right mouth (#55)\n",
    "    \"\"\"\n",
    "    shape = landmark_predictor(image_rgb, face)\n",
    "    \n",
    "    leye = np.array([shape.part(37).x, shape.part(37).y]).reshape(-1, 2)\n",
    "    reye = np.array([shape.part(44).x, shape.part(44).y]).reshape(-1, 2)\n",
    "    nose = np.array([shape.part(30).x, shape.part(30).y]).reshape(-1, 2)\n",
    "    lmouth = np.array([shape.part(49).x, shape.part(49).y]).reshape(-1, 2)\n",
    "    rmouth = np.array([shape.part(55).x, shape.part(55).y]).reshape(-1, 2)\n",
    "    \n",
    "    pts = np.concatenate([leye, reye, nose, lmouth, rmouth], axis=0)\n",
    "    return pts\n",
    "\n",
    "\n",
    "def align_and_crop_face(img_rgb: np.ndarray, landmarks: np.ndarray, \n",
    "                        outsize: Tuple[int, int] = (224, 224), \n",
    "                        scale: float = 1.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    5ê°œ ëœë“œë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ì •ë ¬ ë° crop\n",
    "    \"\"\"\n",
    "    target_size = [112, 112]\n",
    "    dst = np.array([\n",
    "        [30.2946, 51.6963],\n",
    "        [65.5318, 51.5014],\n",
    "        [48.0252, 71.7366],\n",
    "        [33.5493, 92.3655],\n",
    "        [62.7299, 92.2041]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    if target_size[1] == 112:\n",
    "        dst[:, 0] += 8.0\n",
    "\n",
    "    dst[:, 0] = dst[:, 0] * outsize[0] / target_size[0]\n",
    "    dst[:, 1] = dst[:, 1] * outsize[1] / target_size[1]\n",
    "\n",
    "    target_size = outsize\n",
    "\n",
    "    margin_rate = scale - 1\n",
    "    x_margin = target_size[0] * margin_rate / 2.\n",
    "    y_margin = target_size[1] * margin_rate / 2.\n",
    "\n",
    "    dst[:, 0] += x_margin\n",
    "    dst[:, 1] += y_margin\n",
    "\n",
    "    dst[:, 0] *= target_size[0] / (target_size[0] + 2 * x_margin)\n",
    "    dst[:, 1] *= target_size[1] / (target_size[1] + 2 * y_margin)\n",
    "\n",
    "    src = landmarks.astype(np.float32)\n",
    "\n",
    "    tform = trans.SimilarityTransform()\n",
    "    tform.estimate(src, dst)\n",
    "    M = tform.params[0:2, :]\n",
    "\n",
    "    aligned = cv2.warpAffine(img_rgb, M, (target_size[1], target_size[0]))\n",
    "    \n",
    "    if outsize is not None:\n",
    "        aligned = cv2.resize(aligned, (outsize[1], outsize[0]))\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "\n",
    "def extract_aligned_face_fast(img_rgb: np.ndarray, res: int = 224, scale: float = 1.0) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    ì–¼êµ´ ê²€ì¶œ ë° ì •ë ¬ (ì¶•ì†Œëœ ì´ë¯¸ì§€ì—ì„œ ê²€ì¶œ)\n",
    "    - scale: ì´ë¯¸ì§€ ì¶•ì†Œ ë¹„ìœ¨ (0.8 = 80% í¬ê¸°ë¡œ ì¶•ì†Œ) -> time cost ê°ì†Œ\n",
    "    - ì–¼êµ´ì´ ì—†ìœ¼ë©´ None ë°˜í™˜\n",
    "    \"\"\"\n",
    "    small = cv2.resize(img_rgb, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    faces = face_detector(small, 1)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    \n",
    "    face = max(faces, key=lambda r: r.width() * r.height())\n",
    "    landmarks = get_5_keypoints(small, face)\n",
    "    aligned = align_and_crop_face(small, landmarks, outsize=(res, res))\n",
    "    \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils - Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def blur_score(frame_rgb: np.ndarray) -> float:\n",
    "    \"\"\"í”„ë ˆì„ ì„ ëª…ë„ ì¸¡ì • (í´ìˆ˜ë¡ ì„ ëª…)\"\"\"\n",
    "    gray = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def importance_frame_indices(\n",
    "    frames: list[np.ndarray],\n",
    "    num_frames: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ì¤‘ìš”ë„ ê¸°ë°˜ í”„ë ˆì„ ìƒ˜í”Œë§\n",
    "    - ì–¼êµ´ì´ í¬ê³ \n",
    "    - ë¸”ëŸ¬ê°€ ì ê³ \n",
    "    - ì–¼êµ´ ê²€ì¶œì´ ì„±ê³µí•œ í”„ë ˆì„ ìš°ì„ \n",
    "    \"\"\"\n",
    "    if len(frames) <= num_frames:\n",
    "        return np.arange(len(frames))\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "        # ê¸°ë³¸ ì ìˆ˜\n",
    "        score = 0.0\n",
    "\n",
    "        # 1ï¸âƒ£ ì–¼êµ´ ê²€ì¶œ\n",
    "        faces = face_detector(frame, 1)\n",
    "        if len(faces) == 0:\n",
    "            continue  # ì–¼êµ´ ì—†ìœ¼ë©´ importance ë‚®ìŒ\n",
    "\n",
    "        # ê°€ì¥ í° ì–¼êµ´ ì‚¬ìš©\n",
    "        face = max(faces, key=lambda r: r.width() * r.height())\n",
    "        face_area = face.width() * face.height()\n",
    "\n",
    "        # 2ï¸âƒ£ blur score\n",
    "        sharpness = blur_score(frame)\n",
    "\n",
    "        # importance score ê³„ì‚°\n",
    "        score = (\n",
    "            0.6 * face_area +      # ì–¼êµ´ í¬ê¸°\n",
    "            0.4 * sharpness        # ì„ ëª…ë„\n",
    "        )\n",
    "\n",
    "        scores.append((idx, score))\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        # fallback: uniform sampling\n",
    "        return uniform_frame_indices(len(frames), num_frames)\n",
    "\n",
    "    # importance ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # ìƒìœ„ num_frames ì„ íƒ\n",
    "    selected = [idx for idx, _ in scores[:num_frames]]\n",
    "\n",
    "    return np.array(sorted(selected))\n",
    "\n",
    "\n",
    "# def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "#     ext = file_path.suffix.lower()\n",
    "\n",
    "#     if ext in IMAGE_EXTS:\n",
    "#         img = cv2.imread(str(file_path))\n",
    "#         if img is None:\n",
    "#             return []\n",
    "#         return [cv2.cvtColor(img, cv2.COLOR_BGR2RGB)]\n",
    "\n",
    "#     if ext in VIDEO_EXTS:\n",
    "#         cap = cv2.VideoCapture(str(file_path))\n",
    "#         total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         if total <= 0:\n",
    "#             cap.release()\n",
    "#             return []\n",
    "\n",
    "#         # ğŸ”¥ 1ë‹¨ê³„: ì¼ë‹¨ í”„ë ˆì„ì„ ë„‰ë„‰íˆ ì½ëŠ”ë‹¤\n",
    "#         frames = []\n",
    "#         for i in range(total):\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "#         cap.release()\n",
    "\n",
    "#         # ğŸ”¥ 2ë‹¨ê³„: importance ê¸°ë°˜ìœ¼ë¡œ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "#         indices = importance_frame_indices(frames, num_frames)\n",
    "\n",
    "#         return [frames[i] for i in indices]\n",
    "\n",
    "#     return []\n",
    "def read_rgb_frames_fast(file_path, num_frames):\n",
    "    cap = cv2.VideoCapture(str(file_path))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = uniform_frame_indices(total, num_frames)\n",
    "\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Face Crop ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        face_imgs: List[Image.Image],\n",
    "        representative_face: Optional[np.ndarray] = None,\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.face_imgs = face_imgs  # PIL Images for inference\n",
    "        self.representative_face = representative_face  # representative face save (RGB numpy)\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "def preprocess_one_with_facecrop(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    íŒŒì¼ í•˜ë‚˜ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰ (ì–¼êµ´ ê²€ì¶œ + crop)\n",
    "    - ë¹„ë””ì˜¤: ì—¬ëŸ¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ, ëŒ€í‘œ 1ì¥ ì €ì¥\n",
    "    - ì´ë¯¸ì§€: 1ì¥ì—ì„œ ì–¼êµ´ ê²€ì¶œ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames_fast(file_path, num_frames=num_frames)\n",
    "        \n",
    "        if not frames:\n",
    "            return PreprocessOutput(file_path.name, [], None, \"No frames extracted\")\n",
    "        \n",
    "        face_imgs: List[Image.Image] = []\n",
    "        representative_face: Optional[np.ndarray] = None\n",
    "        \n",
    "        for i, rgb in enumerate(frames):\n",
    "            aligned_face = extract_aligned_face_fast(rgb, res=224, scale=1.0)\n",
    "            \n",
    "            if aligned_face is not None:\n",
    "                face_imgs.append(Image.fromarray(aligned_face))\n",
    "                \n",
    "                if representative_face is None:\n",
    "                    representative_face = aligned_face\n",
    "            \n",
    "            aligned_face = extract_aligned_face_fast(rgb, res=224, scale=1.0)\n",
    "\n",
    "        if not face_imgs:\n",
    "            return PreprocessOutput(file_path.name, [], None, \"No face detected\")\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, face_imgs, representative_face, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], None, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preprocessing & Saving Face Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 500\n",
      "Cropped faces will be saved to: output/cropped_faces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   2%|â–         | 11/500 [01:59<1:28:43, 10.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m saved_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_one_with_facecrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     preprocess_results[out\u001b[38;5;241m.\u001b[39mfilename] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo face\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39merror:\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mpreprocess_one_with_facecrop\u001b[0;34m(file_path, num_frames)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03míŒŒì¼ í•˜ë‚˜ì— ëŒ€í•œ ì „ì²˜ë¦¬ ìˆ˜í–‰ (ì–¼êµ´ ê²€ì¶œ + crop)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m- ë¹„ë””ì˜¤: ì—¬ëŸ¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ, ëŒ€í‘œ 1ì¥ ì €ì¥\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m- ì´ë¯¸ì§€: 1ì¥ì—ì„œ ì–¼êµ´ ê²€ì¶œ\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mread_rgb_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m frames:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m PreprocessOutput(file_path\u001b[38;5;241m.\u001b[39mname, [], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo frames extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m, in \u001b[0;36mread_rgb_frames\u001b[0;34m(file_path, num_frames)\u001b[0m\n\u001b[1;32m     90\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# ğŸ”¥ 2ë‹¨ê³„: importance ê¸°ë°˜ìœ¼ë¡œ ì¸ë±ìŠ¤ ì„ íƒ\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mimportance_frame_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [frames[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mimportance_frame_indices\u001b[0;34m(frames, num_frames)\u001b[0m\n\u001b[1;32m     31\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 1ï¸âƒ£ ì–¼êµ´ ê²€ì¶œ\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# ì–¼êµ´ ì—†ìœ¼ë©´ importance ë‚®ìŒ\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "if SAVE_CROPS:\n",
    "    print(f\"Cropped faces will be saved to: {CROP_SAVE_DIR}\")\n",
    "\n",
    "preprocess_results: Dict[str, PreprocessOutput] = {}\n",
    "no_face_files: List[str] = []\n",
    "saved_count = 0\n",
    "\n",
    "for file_path in tqdm(files, desc=\"Preprocessing\"):\n",
    "    out = preprocess_one_with_facecrop(file_path)\n",
    "    preprocess_results[out.filename] = out\n",
    "\n",
    "    if out.error and \"No face\" in out.error:\n",
    "        no_face_files.append(out.filename)\n",
    "\n",
    "    if SAVE_CROPS and out.representative_face is not None:\n",
    "        save_name = Path(out.filename).stem + \".jpg\"\n",
    "        save_path = CROP_SAVE_DIR / save_name\n",
    "        cv2.imwrite(\n",
    "            str(save_path),\n",
    "            cv2.cvtColor(out.representative_face, cv2.COLOR_RGB2BGR)\n",
    "        )\n",
    "        saved_count += 1\n",
    "\n",
    "print(\"\\nPreprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif no_face_files:\\n    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\\n    for f in no_face_files[:30]:\\n        print(f\"  - {f}\")\\n    if len(no_face_files) > 30:\\n        print(f\"  ... and {len(no_face_files) - 30} more\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of failed files - If you want to see files with no detected faces, uncomment below\n",
    "'''\n",
    "if no_face_files:\n",
    "    print(f\"\\n=== Files with no face detected ({len(no_face_files)}) ===\")\n",
    "    for f in no_face_files[:30]:\n",
    "        print(f\"  - {f}\")\n",
    "    if len(no_face_files) > 30:\n",
    "        print(f\"  ... and {len(no_face_files) - 30} more\")\n",
    "'''\n",
    "# results : missing faces data = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    \"\"\"PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ Fake probability ì¶”ë¡ \"\"\"\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        batch_probs = F.softmax(logits, dim=1)[:, 1]  # Real probability (id2label: 0=Fake, 1=Real)\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1922229.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results: Dict[str, float] = {}\n",
    "\n",
    "for filename, out in tqdm(preprocess_results.items(), desc=\"Inference\"):\n",
    "    if out.face_imgs:\n",
    "        probs = infer_fake_probs(out.face_imgs)\n",
    "        results[filename] = float(np.percentile(probs, 10)) if probs else 0.0\n",
    "    else:\n",
    "        # ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ 0 (Realë¡œ ì²˜ë¦¬) -> basic logic\n",
    "        results[filename] = 0.0\n",
    "print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: output/submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(\"/Users/taehayeong/Desktop/dataset-face/open/sample_submission.csv\")\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
